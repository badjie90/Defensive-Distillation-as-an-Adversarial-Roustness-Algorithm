# Defensive-Distillation-as-an-Adversarial-Roustness-Algorithm

Contained within this repository is a defensive distillation project written in the Python programming language. The architecture of this project is such that it is adaptable and can be leverage to robustify any deep neural network, with any image dataset.

To make use of this project and make easy for yourself, please use the Defensive_distillation.ipynb

The original concept of defensive distillation as an adversarial robustness algorithm was first introduced by Nicolas Papernot of The Pennsylvania State University, in a paper accessible via this [link](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Distillation+as+a+Defense+to+Adversarial+Perturbations+against+Deep+Neural+Networks&btnG=). However, we have made a notable advancement to this method by addressing the issue in which attackers could reverse-engineer both models to discover fundamental exploits. Moreover, we have implemented training data filtering techniques to mitigate poisoning attacks, in which the initial training database is corrupted by a malicious actor. 

To overcome the challenge posed by poison adversarial examples in the training data, we incorporated an advanced denoising autoencoder (DAE) to remove the adversarial examples from the training dataset. By doing this, we were able to use only the clean images for training the teacher model, thus addressing a significant limitation of the defensive distillation technique.

The DAE was trained on both the training and validation datasets to learn the underlying patterns in the images, and its performance was evaluated on the validation set using the mean squared error (MSE) metric. The MSE was calculated between the original validation data and the reconstructed validation data generated by the DAE. The lower the MSE value, the more accurate the DAE was at reconstructing the original images.

After validating the DAE's effectiveness in removing the poison data samples, we used it to filter the training dataset and removed the adversarial examples to generate a clean training dataset. This clean dataset was then used to train the teacher model using the defensive distillation technique. The resulting model was more robust to adversarial attacks and less susceptible to poisoning attacks. By addressing these limitations, our approach significantly improved the security of deep neural networks, making them more suitable for real-world applications.

To expound further, our approach consists of generating a teacher model that is trained on the clean and reconstructed training dataset from the DAE. The teacher model's predictions are then used to create a new dataset, which is used to train the student model. This process of distillation provides a more robust and secure model that is less susceptible to adversarial attacks. Additionally, we introduce a novel technique that ensembles multiple adversarial attacks to assess the model's resilience, providing a more comprehensive evaluation.

Overall, this repository offers a cutting-edge approach to improving the security of deep neural networks, addressing issues that have long plagued the field. With this project, one can easily integrate our defensive distillation methodology into their existing models, enhancing their robustness and mitigating the risks of adversarial attacks.




## To download and convert the dataset into pickle file:

### run the following command in the project directory

$ ./downloadDataset.sh This will download the zip file and unzip in the current directory as GTSRB. The data formatting of images is given in Readme-Images.txt

To generate pickle files using GTSRB dataset change the parameters according to the requirement

dataGen = datasetGenerator(nClasses=5, nTrainSamples=800, nTestSamples=100, nValidateSamples=100,imageSize=[28, 28])

nClasses - no of classes nTrainSamples - no of Training samples nTestSamples - no of test samples nValidateSamples - no of validation samples imageSize - size of 2D image matrix to be resized into. $ python datasetGenerator.py

This will generate the following files

├── info.txt

├── test.p

├── train.p

└── validate.p
